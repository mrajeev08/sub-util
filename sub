#!/usr/bin/env Rscript
suppressPackageStartupMessages(library("argparse"))

# directory to cd to
dir <- unlist(strsplit(getwd(), "/"))
dir <- dir[length(dir)]

# find rsync script and use it to push up to remote
rsync <- grep("rsync", list.files("bash"), value = TRUE)

# get user name & cluster from rsync
usr <- unlist(strsplit(grep("user:=", readLines(paste0("bash/", rsync)), value = TRUE), ":="))[2]
cluster <- unlist(strsplit(grep("cluster:=", readLines(paste0("bash/", rsync)), value = TRUE), ":="))[2]
target <- paste0(usr, "@", cluster)
to_email <- paste0(usr, "@princeton.edu")

# create parser object
parser <- ArgumentParser(prog="sub", description="Utility for submitting jobs (in this case an R script) to remote cluster. This uses an rsync to sync from the local directory to the remote directory. You will need to make a directory `bash` where all the bash & slurm scripts will be stored. In this directory, create an Rsync script with 'rsync' in it's name, and that has two commented lines specifying your Princeton email/username and the target cluster (see example/bash/rsync.sh). In each Rscript you want to run, you can either pass the directories you want to sync from (syncfrom) from and sync to (syncto) as command line arguments or you can include them in the script itself. An example is included in this directory.")

# specify our desired options
# by default ArgumentParser will add a help option
# use sub -h/ --help 

# For gpu
parser$add_argument("--noGPU", action="store_true", default=TRUE, help="Do not write GPU line, pass -g or --gpu to write the GPU line")
parser$add_argument("-g", "--gpu", action="store_false", dest="noGPU")

# For multiple nodes with MPI vs single node
parser$add_argument("-mn", "--multinode", action="store_true", default=TRUE, help="Do not write the node line (multinode jobs with rMPI)")
parser$add_argument("-sn", "--single", action="store_false", dest="multinode", help="Write the node line for running job on single node")

parser$add_argument("-t", "--time", type="integer", default=24, help="Number of hours to run",
dest = "time", metavar = "")
parser$add_argument("-n", "--ntasks", type="integer", default=1, dest="ntasks", help="Number of tasks across all nodes", metavar="")
parser$add_argument("-mem", "--memCPU", type = "integer", default=4000, dest="memCPU", help="Megabyte of memory per CPU", metavar ="")

parser$add_argument("-sp", "--scriptPath", help="Name of R script", dest="scriptPath", default="myscript.R", metavar = "")
parser$add_argument("-jn", "--jobName", help="Name of job/slurm script", dest="jobName", default="myjob", metavar = "")

parser$add_argument("-wt", "--wait", help="time interval to check if job complete", dest = "wait", default="10m", metavar = "")
parser$add_argument("-@", "--email", action="store_true", help="whether to email or not", default = TRUE)
parser$add_argument("-n@", "--noemail", action="store_false", dest="email", help="don't email")

# For writing local bash script
parser$add_argument("-st", "--syncto", help = "Directory to pull TO, if NULL will look in Rscript passed for syncto", default= 'NULL', dest="syncto", metavar = "")
parser$add_argument("-sf", "--syncfrom", help = "Remote directory to pull FROM, if NULL will look in Rscript passed for syncfrom", default = 'NULL', dest="syncfrom",metavar = "")

parser$add_argument("--runsync", action="store_true", default=TRUE, help="Rsyncs up, submits the job, and then pulls down results. To only pull down results pass -no or --norun and the script path (see -sp argument)")
parser$add_argument("-no", "--norun", action="store_false", dest="runsync")

parser$add_argument("-md", "--modules", help = "Other modules to load in slurm script, should be passed as string of module names separated by a space and in quotes", dest = "mods", default = "", metavar = "")

# get command line options, if help option encountered print help and exit,
# otherwise if options not found on command line then set defaults,
args <- parser$parse_args()

hours <- trimws(toString(args$time))
ntasks <- trimws(toString(args$ntasks))
memCPU <- trimws(toString(args$memCPU))
jobName <- noquote(trimws(toString(args$jobName)))
scriptPath <- trimws(toString(args$scriptPath))
wait <- trimws(toString(args$wait))
mods <- toString(args$mods)

if(is.null(args$syncto)) {
  syncto <- trimws(toString(args$syncto))
} else {
  syncto <- system2("grep", args = paste("syncto ", scriptPath, " | cut -f 2 -d -", sep = ""),
                        stdout = TRUE)
  syncto <- unlist(strsplit(syncto, "\""))[2]
  print(paste("will pull to: ", syncto))
}
  
if(is.null(args$syncfrom)) {
  syncfrom <- trimws(toString(args$syncfrom))
} else {
  syncfrom <- system2("grep", args = paste("syncfrom ", scriptPath, " | cut -f 2 -d -", sep = ""),
                      stdout = TRUE)
  syncfrom <- unlist(strsplit(syncfrom, "\""))[2]
  print(paste("will pull from: ", syncfrom))
}
  
if( !args$runsync ) {
  system2("rsync", paste("-rLvzt", syncfrom, syncto, sep = " "))
} else {
  # parse from Rscript
  sink(paste("bash/", jobName, ".slurm", sep = ""))
  cat("#!/bin/bash\n")
  cat(paste("#SBATCH --job-name=", jobName,"	 # create a short name for your job\n", sep=""))
  
  if ( !args$multinode ) { cat("#SBATCH --nodes=1		      # node count\n") }
  cat(paste("#SBATCH --ntasks=", ntasks, "	        # total number of tasks across all nodes\n", sep=""))
  cat("#SBATCH --cpus-per-task=1	 # cpu-cores per task (>1 if multithread tasks)\n")
  cat(paste("#SBATCH --mem-per-cpu=", memCPU, "	 # mem per CPU\n", sep=""))
  if ( !args$noGPU ) { cat("#SBATCH --gpus=gres:1            # number of gpus per node\n") }
  cat(paste("#SBATCH --time=", hours, ":00:00          # total run time limit (HH:MM:SS)\n", sep=""))
  
  if ( args$email ) { 
  cat("#SBATCH --mail-type=begin        # send mail when process begins\n")
  cat("#SBATCH --mail-type=end	        # send email when job ends\n")
  cat(paste("#SBATCH --mail-user=", to_email, "         # email\n", sep=""))
  }
  cat("\n")
  
  if(args$mods != "") {
      cat(paste("module load", mods, "\n"))
  }
  
  if ( args$multinode ) {
      cat("export OMPI_MCA_btl='tcp,self,sm'\n")
      cat("module load openmpi/gcc/2.0.2\n")
      cat(paste("srun Rscript ", scriptPath, "\n", sep = ""))
  } else {
      cat(paste("Rscript ", scriptPath, "\n", sep = ""))
  }
  sink()
  
  sink(paste("bash/", jobName, ".sh", sep = ""))
  cat("#!/bin/bash\n")
  cat(paste("ssh -T ", target, " <<HERE\n", sep = ""))
  cat(paste("    cd ", dir, " # change to repo\n"))
  cat(paste("    jid=\\$(sbatch bash/", jobName, ".slurm | cut -c 21-)\n", sep = ""))
  cat("    echo \"Here's the job id: \\$jid\"\n")
  cat("    jstat=\\$(sacct -j \"\\$jid\" -u mrajeev | head -n 3)\n")
  cat("    echo \"Here's the job stat: \\$jstat\"\n")
  cat("    until grep -q \"COMPLETED\\|FAILED\\|CANCELLED\" <<< \\$jstat  # if completed or failed\n")
  cat("    do\n")
  cat("        echo waiting   # updating\n")
  cat("        jstat=\\$(sacct -j \"\\$jid\" -u mrajeev | head -n 3)\n")
  cat("        echo \"Here's the job stat: \\$jstat\"\n")
  cat(paste("        sleep ", wait, " # time to sleep for (base it on how long the job should take)\n", sep = ""))
  cat("    done\n")
  cat("    if grep -q \"FAILED\\CANCELLED\" <<< \\$jstat\n")
  cat("    then\n        echo \"Failed or cancelled\"\n")
  cat("        exit\n")
  cat("    else\n        logout\n    fi\nHERE\n")
  cat("        sleep 1m    # sleep again as sometimes takes a while to write output\n")
  cat(paste("        rsync -rLvzt", syncfrom, syncto, "\n", sep = " "))
  sink()
  
  # first rsync up
  system2("echo", paste0("Sync script: ", rsync))
  system2("bash", paste0("bash/", rsync))
   
  # bash job.sh (running locally!)
  system2("bash", args = paste("bash/", jobName, ".sh", sep = ""))
}

